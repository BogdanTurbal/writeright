{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BogdanTurbal/writeright/blob/main/ScrabbleGAN_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtOBKRW5xWbb"
      },
      "source": [
        "# ScrabbleGAN DEMO (TF 2.X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "more information: https://github.com/Nikolai10/scrabble-gan"
      ],
      "metadata": {
        "id": "7coV-TRwfts7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ],
      "metadata": {
        "id": "YqBDOXCrf6Xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Project"
      ],
      "metadata": {
        "id": "FGoPsiIifz0q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2V7M2tfS3LG"
      },
      "outputs": [],
      "source": [
        "# download project\n",
        "!git clone -b dev https://github.com/Nikolai10/scrabble-gan.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Dataset (For Demonstration Purpose Only)"
      ],
      "metadata": {
        "id": "znuBc1LYf-sx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5q7ZT5OYsgK"
      },
      "outputs": [],
      "source": [
        "# external users: manually download https://drive.google.com/file/d/1duoY9gBmx6quHNGWDlQGKIYO2ubsVZ-y/view?usp=sharing\n",
        "# place files as described in https://github.com/Nikolai10/scrabble-gan (Setup)\n",
        "!mkdir -p /content/scrabble-gan/res/data/iamDB\n",
        "!unzip /content/drive/MyDrive/data.zip -d /content/scrabble-gan/res/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libs"
      ],
      "metadata": {
        "id": "kkhWPioXgGnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "id": "2CjPJyCpVyJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMMjKdbWVAzP"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.extend(['/content/scrabble-gan'])\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "import gin\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_docs.vis.embed as embed\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from src.bigacgan.arch_ops import spectral_norm\n",
        "from src.bigacgan.data_utils import load_prepare_data, train, make_gif, load_random_word_list\n",
        "from src.bigacgan.net_architecture import make_generator, make_discriminator, make_recognizer, make_gan\n",
        "from src.bigacgan.net_loss import hinge, not_saturating\n",
        "\n",
        "gin.external_configurable(hinge)\n",
        "gin.external_configurable(not_saturating)\n",
        "gin.external_configurable(spectral_norm)\n",
        "\n",
        "from src.dinterface.dinterface import init_reading"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init Config Params"
      ],
      "metadata": {
        "id": "9_-_JVScgJ2H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loqTm1kTVXzQ"
      },
      "outputs": [],
      "source": [
        "@gin.configurable\n",
        "def setup_optimizer(g_lr, d_lr, r_lr, beta_1, beta_2, loss_fn, disc_iters):\n",
        "    generator_optimizer = tf.keras.optimizers.Adam(learning_rate=g_lr, beta_1=beta_1, beta_2=beta_2)\n",
        "    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=d_lr, beta_1=beta_1, beta_2=beta_2)\n",
        "    recognizer_optimizer = tf.keras.optimizers.Adam(learning_rate=r_lr, beta_1=beta_1, beta_2=beta_2)\n",
        "    return generator_optimizer, discriminator_optimizer, recognizer_optimizer, loss_fn, disc_iters\n",
        "\n",
        "\n",
        "@gin.configurable('shared_specs')\n",
        "def get_shared_specs(epochs, batch_size, latent_dim, embed_y, num_gen, kernel_reg, g_bw_attention, d_bw_attention):\n",
        "    return epochs, batch_size, latent_dim, embed_y, num_gen, kernel_reg, g_bw_attention, d_bw_attention\n",
        "\n",
        "\n",
        "@gin.configurable('io')\n",
        "def setup_io(base_path, checkpoint_dir, gen_imgs_dir, model_dir, raw_dir, read_dir, input_dim, buf_size, n_classes,\n",
        "             seq_len, char_vec, bucket_size):\n",
        "    gen_path = base_path + gen_imgs_dir\n",
        "    ckpt_path = base_path + checkpoint_dir\n",
        "    m_path = base_path + model_dir\n",
        "    raw_dir = base_path + raw_dir\n",
        "    read_dir = base_path + read_dir\n",
        "    return input_dim, buf_size, n_classes, seq_len, bucket_size, ckpt_path, gen_path, m_path, raw_dir, read_dir, char_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1YZUdRCVnfZ"
      },
      "outputs": [],
      "source": [
        "# init params\n",
        "gin.parse_config_file('/content/scrabble-gan/src/scrabble_gan.gin')\n",
        "epochs, batch_size, latent_dim, embed_y, num_gen, kernel_reg, g_bw_attention, d_bw_attention = get_shared_specs()\n",
        "in_dim, buf_size, n_classes, seq_len, bucket_size, ckpt_path, gen_path, m_path, raw_dir, read_dir, char_vec = setup_io()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Preprocess Dataset"
      ],
      "metadata": {
        "id": "rrEfyRihgNk-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsboZhAIWYBz"
      },
      "outputs": [],
      "source": [
        "# convert IAM Handwriting dataset (words) to GAN format\n",
        "if not os.path.exists(read_dir):\n",
        "  print('converting iamDB-Dataset to GAN format...')\n",
        "  init_reading(raw_dir, read_dir, in_dim, bucket_size)\n",
        "\n",
        "# load random words into memory (used for word generation by G)\n",
        "random_words = load_random_word_list(read_dir, bucket_size, char_vec)\n",
        "\n",
        "# load and preprocess dataset (python generator)\n",
        "train_dataset = load_prepare_data(in_dim, batch_size, read_dir, char_vec, bucket_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Composite Model"
      ],
      "metadata": {
        "id": "q-8KMEzLgQCb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9-nGgp-VsOv"
      },
      "outputs": [],
      "source": [
        "# init generator, discriminator and recognizer\n",
        "generator = make_generator(latent_dim, in_dim, embed_y, gen_path, kernel_reg, g_bw_attention, n_classes)\n",
        "discriminator = make_discriminator(gen_path, in_dim, kernel_reg, d_bw_attention)\n",
        "recognizer = make_recognizer(in_dim, seq_len, n_classes + 1, gen_path)\n",
        "\n",
        "# build composite model (update G through composite model)\n",
        "gan = make_gan(generator, discriminator, recognizer, gen_path)\n",
        "\n",
        "# init optimizer for both generator, discriminator and recognizer\n",
        "generator_optimizer, discriminator_optimizer, recognizer_optimizer, loss_fn, disc_iters = setup_optimizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Optimizers + Checkpoint-Saver\n"
      ],
      "metadata": {
        "id": "WJ08-2b6gUgu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BroxWH3ZV-5y"
      },
      "outputs": [],
      "source": [
        "# purpose: save and restore models\n",
        "checkpoint_prefix = os.path.join(ckpt_path, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                     discriminator_optimizer=discriminator_optimizer,\n",
        "                                     recognizer_optimizer=recognizer_optimizer,\n",
        "                                     generator=generator,\n",
        "                                     discriminator=discriminator,\n",
        "                                     recognizer=recognizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Training"
      ],
      "metadata": {
        "id": "w0coxgiyge4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**note:** If you use the [free Colab version](https://colab.research.google.com/signup), you should first reduce the number of epochs (e.g. epochs=5) to not exceed the 12h time limit.  "
      ],
      "metadata": {
        "id": "Kgn4kkPZaKbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST8zVGl_WLGi"
      },
      "outputs": [],
      "source": [
        "# reuse this seed + labels overtime to visualize progress in the animated GIF\n",
        "seed = tf.random.normal([num_gen, latent_dim])\n",
        "random_bucket_idx = random.randint(4, bucket_size - 1)\n",
        "labels = np.array([random.choice(random_words[random_bucket_idx]) for _ in range(num_gen)], np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2jzbQxuWQKd"
      },
      "outputs": [],
      "source": [
        "# start training\n",
        "train(train_dataset, generator, discriminator, recognizer, gan, checkpoint, checkpoint_prefix, generator_optimizer,\n",
        "          discriminator_optimizer, recognizer_optimizer, [seed, labels], buf_size, batch_size, epochs, m_path,\n",
        "          latent_dim, gen_path, loss_fn, disc_iters, random_words, bucket_size, char_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v_53fDrCrJJ"
      },
      "outputs": [],
      "source": [
        "# use imageio to create an animated gif using the images saved during training.\n",
        "make_gif(gen_path)\n",
        "embed.embed_file(gen_path + 'biggan.gif')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Inference On Your Data"
      ],
      "metadata": {
        "id": "MtZAMYUzlMR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_saved_model = '/content/scrabble-gan/res/out/big_ac_gan/model/generator_' + str(epochs)\n",
        "\n",
        "# number of samples to generate\n",
        "n_samples = 10\n",
        "# your sample string\n",
        "sample_string = 'machinelearning'\n",
        "\n",
        "# load trained model\n",
        "imported_model = tf.saved_model.load(path_to_saved_model)\n",
        "\n",
        "# inference loop\n",
        "for idx in range(1):\n",
        "  fake_labels = []\n",
        "  words = [sample_string] * 10\n",
        "  noise = tf.random.normal([n_samples, latent_dim])\n",
        "  \n",
        "  # encode words\n",
        "  for word in words:\n",
        "    fake_labels.append([char_vec.index(char) for char in word])\n",
        "  fake_labels = np.array(fake_labels, np.int32)\n",
        "\n",
        "  # run inference process\n",
        "  predictions = imported_model([noise, fake_labels], training=False)\n",
        "  # transform values into range [0, 1]\n",
        "  predictions = (predictions + 1) / 2.0\n",
        "\n",
        "  # plot results\n",
        "  for i in range(predictions.shape[0]):\n",
        "    plt.subplot(10, 1, i + 1)\n",
        "    plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "    # plt.text(0, -1, \"\".join([char_vec[label] for label in fake_labels[i]]))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fXzp579rlOe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IGyxKq6aXEg0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
